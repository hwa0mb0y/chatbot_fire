import streamlit as st
import warnings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="í™”í•™ ë¬¼ì§ˆ í™”ì¬ ì‚¬ê³  ì „ë¬¸ ì±—ë´‡", page_icon="ğŸ¤–")
st.title("í™”í•™ ë¬¼ì§ˆ í™”ì¬ ì‚¬ê³  ì „ë¬¸ ì±—ë´‡")

# OpenAI API í‚¤ ì„¤ì •
openai_api_key = st.sidebar.text_input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
@st.cache_resource
def load_vector_store():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    vector_store = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    return vector_store

vector_store = load_vector_store()

# ì±—ë´‡ ì´ˆê¸°í™”
@st.cache_resource
def initialize_chatbot(_openai_api_key):
    llm = ChatOpenAI(
        temperature=0.1,
        max_tokens=500,
        openai_api_key=_openai_api_key
    )
    
    prompt_template = """
    ë‹¹ì‹ ì€ ì „ë¬¸ ë¶„ì•¼ì— ëŒ€í•œ ì§€ì‹ì´ í’ë¶€í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”. 
    ê°€ëŠ¥í•œ í•œ ë§ì€ ê´€ë ¨ ì •ë³´ë¥¼ í¬í•¨í•˜ê³ , ì˜ˆì‹œë‚˜ ì„¤ëª…ì„ ì¶”ê°€í•˜ì—¬ ë‹µë³€ì„ í’ë¶€í•˜ê²Œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.

    ë§¥ë½: {context}
    
    ì§ˆë¬¸: {question}
    
    ë‹µë³€:
    """
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )
    
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": PROMPT}
    )
    return chain

# ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    if not openai_api_key:
        st.info("OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    # ì±—ë´‡ ì‘ë‹µ ìƒì„±
    chain = initialize_chatbot(openai_api_key)
    response = chain({"question": prompt, "chat_history": [(m["role"], m["content"]) for m in st.session_state.messages]})
    
    with st.chat_message("assistant"):
        st.markdown(response["answer"])
    
    st.session_state.messages.append({"role": "assistant", "content": response["answer"]})

# ì‚¬ì´ë“œë°”ì— ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
with st.sidebar:
    if st.session_state.messages and "answer" in response:
        st.subheader("ì°¸ê³  ë¬¸ì„œ")
        for doc in response["source_documents"]:
            st.write(doc.page_content[:200] + "...")
